{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"dDAn1YGh8vo7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661259192006,"user_tz":240,"elapsed":16617,"user":{"displayName":"Soo Ji","userId":"15954628017250447588"}},"outputId":"32288dfe-889a-43b5-976b-32543c6d6da2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xt1lFqwc8yEY"},"outputs":[],"source":["!pip install pyreadr\n","\n","import pyreadr\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","from numpy import array\n","from numpy.random import uniform\n","from numpy import hstack\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","import math as m\n","from keras.layers import Dropout\n","from keras.callbacks import EarlyStopping\n","import copy as c\n","from  google.colab import drive\n","from scipy.stats import spearmanr\n","from scipy.ndimage.filters import gaussian_filter1d\n","from scipy.signal import argrelmax\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import multilabel_confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics\n","import logging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWsOwJeM84_k"},"outputs":[],"source":["fs = 60  # training\n","hours_ = 16  # hours info\n","bootstrap_rep =2000\n","\n","\n","data_folder=\"/content/drive/MyDrive/Kyoto/data\"\n","# data_folder = \"F:\\\\KyotoTemporal\\\\out\\\\MLE\\\\test\\\\\"\n","# data_folder=\"F:\\\\KyotoTemporal\\\\out\\\\MLE_TS_1\\\\01\\\\\"\n","# out_folder = \"F:\\\\KyotoTemporal\\\\out\\\\RNN_out\\\\RiskEstimate\\\\\"\n","out_folder = \"/content/drive/MyDrive/Kyoto\"\n","## Monthly data store\n","# base_folder = \"F:\\\\KyotoTemporal\\\\out\\\\RNN_out\\\\Fcast_Actual\\\\\"\n","\n","\n","day_list = []\n","\n","\n","#create a logger\n","# logger = logging.getLogger()\n","\n","for folder in os.listdir(data_folder):\n","    df3=pd.DataFrame()\n","    d = os.path.join('/content/drive/MyDrive/Kyoto/data', folder)\n","    #     df =pd.read_csv(d)\n","    result = pyreadr.read_r(d)\n","    \n","    print(result.keys())\n","    \n","    # input_data = result[\"user_ts\"]  # extract the pandas data frame for object c6 --> MLE data\n","    input_data = result[\"input\"]  # extract the pandas data frame for object c6\n","\n","    d1 = input_data.drop([\"i\"], axis=1)\n","\n","    # day = pd.to_numeric((d.split('1_')[1]).split('.')[0]) ---> for MLE data\n","    day = pd.to_numeric(d.split('2015')[1][2:4])\n","    day_list.append(day)\n","    out_perm = np.array([])\n","  # log file\n","    log_filename = str.format('mylog%d.log' % day)\n","    logging.basicConfig(filename=log_filename, level=logging.INFO)\n","\n","    np_filter_d1 = d1.fillna(method='bfill')  # ffill - forward-fill propagate inplace=False\n","\n","    ##### ---------------  for TESTING ONLY\n","    # filtered_d1 = c.deepcopy(np_filter_d1.iloc[0:5000,:])\n","\n","  ##### ---------------  for TESTING ONLY\n","\n","\n","    ###### ------------ Testing for Entire data\n","    ###### ------------\n","    filtered_d1 = c.deepcopy(np_filter_d1)\n","    ###### ------------\n","    ###### ------------ Testing for Entire data\n","    \n","\n","# 16 hours of data for training\n","    # train_size=int(fs*hours_*60) --> for MLE 1sec\n","    train_size=int(fs*hours_)\n","    col_len = filtered_d1.shape[1]\n","    \n","    # seperate training and testing data\n","    tr_df = c.deepcopy(filtered_d1[0:train_size])\n","    tst_df = c.deepcopy(filtered_d1[train_size:])\n","    \n","# feature varianve ----------------------------------------------------\n","  # all actual variable\n","    combine_actual_df =pd.concat([tr_df, tst_df], axis=0)\n","  # original actal feature variance\n","    # actual_df_var = combine_actual_df.iloc[:,0:37].var(axis=1)  #-------> Original for MLE\n","    actual_df_var = combine_actual_df.iloc[:,0:85].var(axis=1)  #-------> modified July for DWT_PE\n","  # integration of actual feature variance and class such as session, normal, KA, UA\n","    actual_df = pd.concat([actual_df_var, combine_actual_df.iloc[:,37:col_len]], axis=1)\n","    \n","    # scaling to (0,1)\n","    train_df = c.deepcopy(tr_df)\n","    scalers={}\n","    for i in tr_df.columns:\n","        scaler = MinMaxScaler(feature_range=(0,1))\n","        s_s = scaler.fit_transform(train_df[i].values.reshape(-1,1))\n","        s_s=np.reshape(s_s,len(s_s))\n","        scalers['scaler_'+ i] = scaler\n","        train_df[i]=s_s\n","    test_df = c.deepcopy(tst_df)  \n","    for i in tr_df.columns:\n","        scaler = scalers['scaler_'+i]\n","        s_s = scaler.transform(test_df[i].values.reshape(-1,1))\n","        s_s=np.reshape(s_s,len(s_s))\n","        scalers['scaler_'+i] = scaler\n","        test_df[i]=s_s\n","\n","\n","  # train and test data combine\n","    data_scaled = pd.concat([train_df, test_df], axis=0)\n","\n","    \n","# prepare for writing output -------------------------------\n","    os.chdir(out_folder) #change the month directory\n","    # print('Completed current folder --',out_folder)\n","    month_folder = (d.split('2015')[1][0:2])\n","\n","\n","    # create a folder for month\n","    if not os.path.exists(month_folder):\n","      \n","      os.makedirs(month_folder)\n","    # join the month folder to existed directory       \n","    write_dir = os.path.join(out_folder, month_folder)   \n","    os.chdir(write_dir) #change the month directory\n","    print('Completed write --',write_dir)\n","    \n","  ## ------------------------------------------------------------------------------------------------\n","  ##   risk level and threshold\n","  ## ------------------------------------------------------------------------------------------------\n","    level, t, t_status = risk_score_generation_colab(filtered_d1,data_scaled, d, day, write_dir,bootstrap_rep)\n","    \n","    ## ------------------------------------------------------------------------------------------------\n","    # logging.info(\"Done risk_score_generation_colab for entire data \")  \n","     ## ------------------------------------------------------------------------------------------------\n","    # print('------- done 0')\n","    #     # dividing traing and testing with defined train size\n","#     train_df,test_df = data_scaled[0:train_size], data_scaled[train_size:] \n","# class info to be forecasted \n","    out_idx = [d1.columns.get_loc(\"c_ses\"), d1.columns.get_loc(\"Normal\"), d1.columns.get_loc(\"KA\"),d1.columns.get_loc(\"UA\")]\n","    \n","    # Set the input_sequence_length length - this is the timeframe used to make a single prediction\n","    input_sequence_length = fs # number of features\n","\n","    # output_sequence_length = len(out_idx) # number of outputs\n","    output_sequence_length = 1 # number of outputs\n","\n","  ## ------------------------------------------------------------------------------------------------\n","  ##   spliting data to features and class\n","  ## ------------------------------------------------------------------------------------------------\n","# \n","    x_train, y_train = partition_dataset(input_sequence_length, output_sequence_length, train_df.values,out_idx)\n","    x_test, y_test = partition_dataset(input_sequence_length, output_sequence_length, test_df.values,out_idx)\n","\n","\n","  ## ------------------------------------------------------------------------------------------------\n","  ##   LSTM model generation \n","  ## ------------------------------------------------------------------------------------------------\n","    n_output_neurons = 4\n","    model = lstm_model_creation(n_output_neurons, x_train)   \n","#     model = Sequential()\n","#     # n_output_neurons = output_sequence_length\n","#     n_output_neurons = 4\n","\n","#     n_input_neurons = x_train.shape[1] * x_train.shape[2]\n","# #     n_input_neurons = x_train.shape[2]\n","#     print(n_input_neurons, x_train.shape[1], x_train.shape[2])\n","#     model.add(LSTM(n_input_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \n","#     model.add(Dropout(0.25))\n","\n","#     model.add(LSTM(int(n_input_neurons/2), return_sequences=False))\n","#     model.add(Dropout(0.25))\n","\n","#     model.add(Dense(20, activation='relu'))\n","#     model.add(Dropout(0.25))\n","\n","#     # model.add(Dense(output_sequence_length))\n","#     model.add(Dense(n_output_neurons))\n","\n","#     model.compile(optimizer='adam', loss='mse',metrics=['accuracy']) \n","    # model.summary()\n","    # Training the model\n","\n","##### ---------------  for TESTING ONLY\n","    # epochs = 10\n","    # batch_size = 2\n","\n","\n","  ## ------------------------------------------------------------------------------------------------\n","  ##   TESTING ONLY\n","  ## ------------------------------------------------------------------------------------------------\n","\n","    epochs = 50\n","    batch_size = 10\n","    early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n","    history = model.fit(x_train, y_train,batch_size=batch_size, \n","                        epochs=epochs, validation_data=(x_test, y_test))\n","    ## ------------------------------------------------------------------------------------------------\n","    ##   write the accuracy for train and validation\n","    ## ------------------------------------------------------------------------------------------------\n","    acc_name = file_name_create(day, '_ACCURACY_train_val')\n","    train_acc=history.history['accuracy']\n","    train_acc = pd.DataFrame(train_acc)\n","    val_acc = history.history['val_accuracy']\n","    val_acc = pd.DataFrame(val_acc)\n","    all_acc = pd.concat([train_acc,val_acc], axis=1)\n","    # all_acc = train_acc +  val_acc\n","    all_acc.columns = ['Train','Validation']                                            \n","\n","    all_acc.to_csv(write_dir + '/' + acc_name,  index=False, header=True)\n","  ## ------------------------------------------------------------------------------------------------\n","    # predict\n","  # ## ------------------------------------------------------------------------------------------------  \n","    # \n","    pred_e1d1=model.predict(x_test)\n","\n","    y_pred = scaler.inverse_transform(pred_e1d1)\n","    y_pred= y_pred.reshape((len(y_test), n_output_neurons))\n","    #reshape_test= y_test.reshape((len(y_test), n_output_neurons))\n","    #inv_y_test = scaler.inverse_transform(reshape_test) #---- not needed\n","    # inverse transform for testing label\n","    # inv_test= scaler.inverse_transform(y_test.reshape(-1,1)).reshape(y_test.shape) \n","    inv_y_test= y_test.reshape((len(y_test), n_output_neurons))\n","\n","    # # actual risk level from data scale     \n","    # act_d_r_l = dy_level_risk[train_size:] # for actual risk level\n","    \n","\n","  \n","    # tst_risk_score = risk_score_calculation(test_df,tst_act_scaled_df,d,out_folder)\n","    # check any numpy array contains Nan value\n","    if  np.isnan(np.sum(y_pred)) : \n","        # y_pred =np.nan_to_num(y_pred) # if true, replace to zero\n","        col_mean = np.nanmean(y_pred, axis=0)\n","        \n","        #Find indices that you need to replace\n","        inds = np.where(np.isnan(y_pred))\n","\n","        #Place column means in the indices. Align the arrays using take\n","        y_pred[inds] = np.take(col_mean, inds[1])\n","    \n","    \n","    ## -------  RMSE accuracy measures  --------------------\n","    for i in range(0,n_output_neurons):\n","        #when target attributes are also scaled\n","        p= mean_squared_error(inv_y_test[:,i], y_pred[:,i], squared=False)\n","        mse= mean_squared_error(inv_y_test[:,i], y_pred[:,i])\n","        mae = mean_squared_error(inv_y_test[:,i], y_pred[:,i])\n","        # when target attributes are not scaled\n","\n","        out_perm = np.hstack((out_perm, p,mae, mse)) \n","#         \n","        del p,mae,mse\n","    \n","    out_perm = np.hstack((day,out_perm))\n","    \n","    \n","    out=pd.DataFrame(out_perm)\n","    out =out.T\n","    out_perm=pd.DataFrame(out_perm)\n","    y_pred=pd.DataFrame(y_pred)\n","    inv_y_test=pd.DataFrame(inv_y_test)\n","\n","    df3 = pd.concat([df3,out], axis=0)\n","\n","  ## ------------------------------------------------------------------------------------------------\n","    # logging.info(\"Done RMSE \")  \n","    ## ------------------------------------------------------------------------------------------------  \n","    tst_row_dim = (test_df.shape)[0]\n","    # tst_scale_var=test_df.iloc[fs+1:tst_row_dim,0:37].var(axis=1) --> MLE\n","    tst_scale_var=test_df.iloc[fs+1:tst_row_dim,0:37].var(axis=1)\n","    \n","    pd.DataFrame(tst_scale_var)\n","    tst_scale_var.reset_index(drop=True,inplace=True)\n","    \n","    pred_row_var = pd.concat([tst_scale_var,y_pred], axis=1)\n","    inv_row_var = pd.concat([tst_scale_var,inv_y_test], axis=1)\n","    \n","  \n","    \n","# write to output\n","    day_folder = (d.split('2015')[1][2:4])\n","    hdr=['fea_var','session','N','A','NA']\n","    fcast_name = file_name_create(day_folder, '_Fcast_values')\n","    pred_row_var.to_csv(write_dir + '/' + fcast_name, index=False,header=hdr) \n","\n","    acual_name = file_name_create(day_folder, '_Actual_values')\n","    inv_row_var.to_csv(write_dir + '/' + acual_name, index=False, header=hdr)  \n","\n","    # target = scale_score['risk']\n","    # target_ecdf_values, y = ecdf_values(target)\n","    # change_values, change_idx = peak_change_V2(target_ecdf_values, str(day), save=False)\n","    # bootstrap_out_idx = bootstrap_out(change_values, change_idx, str(day), btrap_rep)\n","    \n","    # tst_level, t = risk_score_generation_colab(filtered_d1,data_scaled, write_dir, d, day, 'Risk',bootstrap_rep)\n","    tst_act_depnedvarOnly= test_df.iloc[0:(y_pred.shape)[0],37:(test_df.shape)[1]]\n","\n","    # index rearrange from 0\n","    tst_act_depnedvarOnly.reset_index(drop=True,inplace=True)\n","    # test dataset feature variance calculation\n","    tst_scl_var = test_df.iloc[:,0:37].var(axis=1)\n","    pd.DataFrame(tst_scl_var)\n","    tst_scl_var.reset_index(drop=True,inplace=True)\n","    # # combine between the feature var and fcast values\n","    tst_scl_var_df=tst_scl_var.iloc[0:(y_pred.shape)[0]]\n","    tst_scl_var_df=pd.concat([tst_scl_var_df,y_pred],axis=1)\n","    tst_scl_var_df.columns = ['var', 'c_ses', 'Normal', 'KA', 'UA']\n","  \n","\n","  # risk level with fcast values\n","    test_score = fcast_risk_level_colab_2(tst_act_depnedvarOnly,tst_scl_var_df,t,t_status, day, write_dir)\n","\n","    ## ------------------------------------------------------------------------------------------------\n","    # logging.info(\"Done Risk Level with fcast values \")  \n","    ## ------------------------------------------------------------------------------------------------  \n","      \n","\n","    # confusion matrix of risk levels between test depenent variabls and the forecasted variables\n","    Risklevel_performance (level, train_size, test_score, y_pred, d, write_dir,month_folder)\n","\n","     ## ------------------------------------------------------------------------------------------------\n","    # logging.info(\"Done MRiskLevel Performance\")  \n","    ## ------------------------------------------------------------------------------------------------  \n","    \n","    fcast_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n","    if (d.split('2015')[1][2:4]) == '01':\n","        df3.columns = ['Day','S_RMSE','S_MAE','S_MSE','N_RMSE','N_MAE','N_MSE','A_RMSE','A_MAE','A_MSE','UA_RMSE','UA_MAE','UA_MSE']\n","        # fcast_perm_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n","        # fcast_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n","        df3.to_csv(write_dir + '/' + fcast_name,  index=False) \n","          \n","    else:\n","        # fcast_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n","        # df3.to_csv(write_dir + '/' + fcast_perm_name,  index=False, mode='a',header=False)\n","        df3.to_csv(write_dir + '/' + fcast_name,  index=False, mode='a',header=False)\n","\n","## ------------------------------------------------------------------------------------------------\n","    # logging.info(\"Done for day %s \",day)  \n","    ## ------------------------------------------------------------------------------------------------  \n","    "]},{"cell_type":"code","source":["d"],"metadata":{"id":"HwreyEjkwg5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dus2Y_JV9MNU"},"outputs":[],"source":["\n","######----------LSTM Model generation  --------------------------\n","def lstm_model_creation(n_out_nurons,x1_train):\n","\n","\t#LSTM model generation and training\n","    model_g = Sequential()\n","    # n_output_neurons = output_sequence_length\n","    # n_out_nurons = 4\n","\n","    n_in_nurons = x1_train.shape[1] * x1_train.shape[2]\n","#     n_input_neurons = x_train.shape[2]\n","    #print(n_input_neurons, x_train.shape[1], x_train.shape[2])\n","    model_g.add(LSTM(n_in_nurons, return_sequences=True, input_shape=(x1_train.shape[1], x1_train.shape[2]))) \n","    model_g.add(Dropout(0.25))\n","\n","    model_g.add(LSTM(int(n_in_nurons/2), return_sequences=False))\n","    model_g.add(Dropout(0.25))\n","\n","    model_g.add(Dense(20, activation='relu'))\n","    model_g.add(Dropout(0.25))\n","\n","    # model.add(Dense(output_sequence_length))\n","    model_g.add(Dense(n_out_nurons))\n","\n","    model_g.compile(optimizer='adam', loss='mse',metrics=['accuracy']) \n","    # model.summary()\n","    \n","    return model_g\n","\n","###### ---------------------  fcast_risk_level_colab\n","def fcast_risk_level_colab_2(tst_act_depvals,tst_scaled_var,cut_threshold, cut_status, day, write_dir,):\n","\n","\n","  copy_tdf = c.deepcopy(tst_act_depvals)\n","  scale_tscore = c.deepcopy(tst_scaled_var)\n","\n","  col_list = ['KA', 'UA']\n","  scale_tscore['AttackSum'] = copy_tdf[col_list].sum(axis=1)\n","\n","  scale_tscore['ratio_N'] = scale_tscore['Normal'] + scale_tscore['var']\n","  scale_tscore['ratio_A'] = (scale_tscore['UA'] + scale_tscore['KA']) + scale_tscore['var']\n","\n","  idx_n = scale_tscore.columns.get_loc(\"ratio_N\")\n","  idx_a = scale_tscore.columns.get_loc(\"ratio_A\")\n","  idx_s = copy_tdf.columns.get_loc(\"c_ses\")\n","  #     def risk_score(data):\n","  scale_tscore['risk'] = (scale_tscore['ratio_A'] / (scale_tscore['ratio_N'] + scale_tscore['ratio_A'])) * copy_tdf[\n","      'c_ses']\n","  if cut_status != 0: ## there are zero changes\n","    \n","    if cut_status == 1: ## there are zero changes\n","      scale_tscore[\"Level\"] = np.where(scale_tscore[\"risk\"] < cut_threshold, 'L', 'M')\n","    elif cut_status == 2: \n","      scale_tscore[\"Level\"] = np.where(scale_tscore[\"risk\"] < cut_threshold, 'M', 'H')\n","    else:\n","      scale_tscore[\"Level\"] = np.where(scale_tscore[\"risk\"] < cut_threshold, 'H')\n","  else: #cut_status == 0 that is the risk level is L/H or L/M/H\n","\n","    if len(cut_threshold) == 1: # two level\n","          scale_tscore[\"Level\"] = np.where(scale_tscore[\"risk\"] < cut_threshold[0], 'L', 'H')\n","    # elif len(cut_off_value[0]) == 0: \n","    #   ...\n","    else: # three level\n","          min_val = min(cut_threshold)\n","          max_val = max(cut_threshold)\n","          scale_tscore.loc[scale_tscore[\"risk\"] < min_val, \"Level\"] = 'L'\n","          scale_tscore.loc[(scale_tscore['risk'] >= min_val) & (scale_tscore['risk'] < max_val), \"Level\"] = 'M'\n","          scale_tscore.loc[scale_tscore['risk'] >= max_val, \"Level\"] = 'H'\n","\n","  _name = file_name_create(day, '_fcast_risk_level')\n","  scale_tscore.to_csv(write_dir + '/' + _name,  index=False)\n","\n","  return scale_tscore\n","\n","######----------Risklevel_performance  --------------------------\n","def Risklevel_performance (all_level, tr_size, scale_tscore, yhat, day_info, wr_dir,month_info):\n","\t\n","  daily_info = str(day_info).split('2015')[1][2:4]\t\n","  all_risk_level = c.deepcopy(all_level)\n","  yhat_df = c.deepcopy(yhat)\n","  # cm_risk=pd.DataFrame()\n","  cm=pd.DataFrame()\n","\n","  df_level = all_risk_level[tr_size:]\n","  df_level.reset_index(drop=True,inplace=True)\n","  tst_level=df_level.iloc[0:(yhat_df.shape)[0],:]\n","\n","  len_tst_risk = len(pd.unique(tst_level['Level']))\n","  actual_risk = len(pd.unique(all_level['Level']))\n","  daily_info = str(day_info).split('2015')[1][2:4]\n","\n","  if actual_risk != len_tst_risk: ## ----- modified July 31 2022\n","    acc=0\n","    prec=0\n","    recall=0\n","    f1=0\n","    cm = np.hstack((daily_info,acc,prec,recall,f1))\n","\n","    cm1=pd.DataFrame(cm)\n","    cm1=cm1.T\n","    cm1_name = file_name_create(month_info, '_cm_2_risk_level')\n","\n","    if str(day_info).split('2015')[1][2:4] =='01':\n","      cm1.columns=['day','acc','precision','recall','f1']\n","      cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=True)\n","    else:\n","      cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=False,mode='a')\n","\n","  else:   \n","    if len_tst_risk==2:\n","\n","      tn, fp, fn, tp = confusion_matrix(tst_level['Level'], scale_tscore['Level']).ravel()\n","      acc= accuracy_score(tst_level['Level'], scale_tscore['Level']) \n","      prec=metrics.precision_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      recall = metrics.recall_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      #prec=tp/(tp+fp)\n","      #recall=tp/(tp+fn) \n","      f1 = 2 * (prec * recall) / (prec + recall) \n","      #total=(tn+fp+fn+tp)\n","      #acc = (tp+tn)/total\n","      #sen =  tp/(tp+fn)\n","      #spec = tn/(fp+tn)\n","      daily_info = str(day_info).split('2015')[1][2:4] \n","      cm = np.hstack((daily_info,acc,prec,recall,f1))\n","\n","      cm1=pd.DataFrame(cm)\n","      cm1=cm1.T\n","      cm1_name = file_name_create(month_info, '_cm_2_risk_level')\n","\n","      if str(day_info).split('2015')[1][2:4] =='01':\n","        cm1.columns=['day','acc','precision','recall','f1']\n","        cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=True)\n","      else:\n","        cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=False,mode='a')\n","\n","    elif  len_tst_risk==1: # one level \n","      # tn, fp, fn, tp = confusion_matrix(tst_level['Level'], scale_tscore['Level']).ravel()\n","      a_val =  len(tst_level['Level'] == test_score['Level'])\n","      tp=a_val/len(tst_level['Level'])\n","      acc= accuracy_score(tst_level['Level'], scale_tscore['Level']) \n","      prec=metrics.precision_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      recall = metrics.recall_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      #prec=tp/(tp+fp)\n","      #recall=tp/(tp+fn) \n","      f1 = 2 * (prec * recall) / (prec + recall) \n","      #total=(tn+fp+fn+tp)\n","      #acc = (tp+tn)/total\n","      #sen =  tp/(tp+fn)\n","      #spec = tn/(fp+tn)\n","      # daily_info = str(day_info).split('2015')[1][2:4]\n","      cm = np.hstack((daily_info,acc,prec,recall,f1))\n","\n","      cm1=pd.DataFrame(cm)\n","      cm1=cm1.T\n","      cm1_name = file_name_create(month_info, '_cm_2_risk_level')\n","\n","      if str(day_info).split('2015')[1][2:4] =='01':\n","        cm1.columns=['day','acc','precision','recall','f1']\n","        cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=True)\n","      else:\n","        cm1.to_csv(write_dir + '/' + cm1_name,  index=False,header=False,mode='a')\n","    else:\n","\n","      MCM= multilabel_confusion_matrix(tst_level['Level'], scale_tscore['Level'])\n","      # skm.classification_report(tst_level['Level'], scale_tscore['Level'])\n","      acc = accuracy_score(tst_level['Level'], scale_tscore['Level'])\n","      prec=metrics.precision_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      recall = metrics.recall_score(tst_level['Level'], scale_tscore['Level'], average='weighted')\n","      # recall=prec=tp/(tp+fp)\n","      # recall=tp/(tp+fn)\n","      f1 = 2 * (prec * recall) / (prec + recall)\n","      # sen =  tp/(tp+fn)\n","      # spec = tn/(fp+tn)\n","      # daily_info = str(day_info).split('2015')[1][2:4]\n","      cm = np.hstack((daily_info,acc,prec,recall,f1))\n","\n","      cm1=pd.DataFrame(cm)\n","      \n","      cm1=cm1.T\n","      cm_name = file_name_create(month_info, '_cm_2_risk_level')\n","\n","      if str(day_info).split('2015')[1][2:4] =='01':\n","        cm1.columns=['day','acc','precision','recall','f1']\n","        cm1.to_csv(wr_dir + '/' + cm_name,  index=False,header=True)\n","      else:\n","        cm1.to_csv(wr_dir + '/' + cm_name,  index=False,header=False,mode='a')\n","\n","\n","######------  split_series  ------------------------------\n","def split_series(series, n_past, n_future):\n","\n","  #\n","  # n_past ==> no of past observations\n","  #\n","  # n_future ==> no of future observations \n","  #\n","    X, y = list(), list()\n","    for window_start in range(len(series)):\n","        past_end = window_start + n_past\n","        future_end = past_end + n_future\n","        if future_end > len(series):\n","            break\n","    # slicing the past and future parts of the window\n","        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n","        X.append(past)\n","        y.append(future)\n","    return np.array(X), np.array(y)\n","\n","######------partition_dataset  ------------------------------\n","def partition_dataset(input_sequence_length, output_sequence_length, data,index_Close):\n","    x, y = [], []\n","    data_len = data.shape[0]\n","    for i in range(input_sequence_length, data_len - output_sequence_length):\n","        x.append(data[i-input_sequence_length:i,0:index_Close[0]]) #contains input_sequence_length values 0-input_sequence_length * columns\n","        y.append(data[i:i + output_sequence_length, index_Close]) #contains the prediction values for validation (3rd column = Close),  for single-step prediction\n","    \n","    # Convert the x and y to numpy arrays\n","    x = np.array(x)\n","    y = np.array(y)\n","    return x, y\n","\n","\n","# def mape(actual, pred): \n","#     actual, pred = np.array(actual), np.array(pred)\n","#     return np.mean(np.abs((actual - pred) / actual)) * 100\n","\n","######---------percentage_error  ---------------------------\n","def percentage_error(actual, predicted):\n","    res = np.empty(actual.shape)\n","    for j in range(actual.shape[0]):\n","        if actual[j] != 0:\n","            res[j] = (actual[j] - predicted[j]) / actual[j]\n","        else:\n","            res[j] = predicted[j] / np.mean(actual)\n","    return resp\n","\n","# def mean_absolute_percentage_error(y_true, y_pred): \n","# #     return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n","#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n","#     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","######---------mean_absolute_percentage_error  ---------------------------\n","def mean_absolute_percentage_error(y_true, y_pred): \n","#     return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","\n","def fcast_acc_measure(actual, pred):\n","    fcast_acc=[]\n","    len_pred=pred_e1d1.shape[1]\n","    i=0\n","    while i< len_pred:\n","        mae = mean_absolute_error(actual[:,i], pred[:,i])\n","        rmse = m.sqrt(mean_squared_error(actual[:,i], pred[:,i]))\n","        mape= mean_absolute_percentage_error(actual[:,i], pred[:,i])\n","        \n","        p=([mae,rmse,mape])\n","        fcast_acc=np.concatenate([fcast_acc,p])\n","        i += 1\n","    return p\n","\n","\n","######---------risk_level_2  ---------------------------\n","# import copy as c\n","# import numpy as np\n","# # import file_name_create as fc\n","\n","# def risk_level_2(in_data, scale_data,w_2_dir, d_info):\n","\n","#     c_in_data = c.deepcopy(in_data)\n","#     ci_99 = c_in_data.shape[1] - 1  # 99% CI column\n","#     ci_99_val = c_in_data.iloc[:,ci_99]\n","#     diff_idx=np.diff(ci_99_val)\n","#     r_values=[]\n","#     # cut_off_value =  np.where(diff_idx != 0)\n","#     copy_scale_score = c.deepcopy(scale_data)\n","#     risk_score_idx= copy_scale_score.shape[1]-1 #-----> added July 21/22 for DWT_PE\n","# \t## check the difference is all the same  or not\n","#   ## all the same  values either 0, -1,1\n","#     if (len(ci_99_val.unique()) ==1):\n","\n","#       if ci_99_val[0] == 0: ## no changes\n","#         copy_scale_score[\"Level\"] = 'L'\n","#         # r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","#         r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","#       elif  ci_99_val[0] == 1: ## decreasing\n","#         copy_scale_score[\"Level\"] = 'M'\n","#         # r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","#         r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","#       else: ## increasing\n","#         copy_scale_score[\"Level\"] = 'H'\n","#         # r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","#         r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","     \n","#     # elif (len(ci_99_val.unique()) == 2):\n","#     #     cut_off_value =  np.where(diff_idx != 0)\n","#     #     for v in range(0, len(cut_off_value[0])):\n","#     #       r_idx = int(c_in_data.iloc[cut_off_value[0][v]+1,0]) \n","#     #       #print(r_idx)\n","#     #       # get risk score values\n","#     #       #temp_values = copy_scale_score.iloc[r_idx,copy_scale_score.shape[1]-1]\n","#     #       temp_values = copy_scale_score.iloc[r_idx,risk_score_idx] #-----> modified July 21/22 for DWT_PE\n","#     #       r_values.append(temp_values)\n","\n","#     #       # assign risk level to data\n","#     #       if len(cut_off_value[0]) == 1: # two level\n","#     #         copy_scale_score[\"Level\"] = np.where(copy_scale_score[\"risk\"] < r_values[0], 'L', 'H')\n","#     #       else: # three level\n","#     #         min_val = min(r_values)\n","#     #         max_val = max(r_values)\n","#     #         copy_scale_score.loc[copy_scale_score[\"risk\"] < min_val, \"Level\"] = 'L'\n","#     #         copy_scale_score.loc[(copy_scale_score['risk'] >= min_val) & (copy_scale_score['risk'] < max_val), \"Level\"] = 'M'\n","#     #         copy_scale_score.loc[copy_scale_score['risk'] >= max_val, \"Level\"] = 'H'\n","\n","#     #       l_file_name = file_name_create(d_info, '_risklevel')\n","#     #       # l_name = str(day_info) + '_risklevel'\n","#     #       # l_file_name = \"%s.csv\" % l_name\n","#     #       copy_scale_score.to_csv(w_2_dir + '/' + l_file_name, index=False)\n","#     else:  ## different values\n","     \n","#         cut_off_value =  np.where(diff_idx != 0)\n","#         for v in range(0, len(cut_off_value[0])):\n","#           r_idx = int(c_in_data.iloc[cut_off_value[0][v]+1,0]) \n","#           #print(r_idx)\n","#           # get risk score values\n","#           #temp_values = copy_scale_score.iloc[r_idx,copy_scale_score.shape[1]-1]\n","#           temp_values = copy_scale_score.iloc[r_idx,risk_score_idx] #-----> modified July 21/22 for DWT_PE\n","#           r_values.append(temp_values)\n","\n","#           # assign risk level to data\n","#           if len(cut_off_value[0]) == 1: # two level\n","#             copy_scale_score[\"Level\"] = np.where(copy_scale_score[\"risk\"] < r_values[0], 'L', 'H')\n","#           else: # three level\n","#             min_val = min(r_values)\n","#             max_val = max(r_values)\n","#             copy_scale_score.loc[copy_scale_score[\"risk\"] < min_val, \"Level\"] = 'L'\n","#             copy_scale_score.loc[(copy_scale_score['risk'] >= min_val) & (copy_scale_score['risk'] < max_val), \"Level\"] = 'M'\n","#             copy_scale_score.loc[copy_scale_score['risk'] >= max_val, \"Level\"] = 'H'\n","\n","#           l_file_name = file_name_create(d_info, '_risklevel')\n","#           # l_name = str(day_info) + '_risklevel'\n","#           # l_file_name = \"%s.csv\" % l_name\n","#           copy_scale_score.to_csv(w_2_dir + '/' + l_file_name, index=False)\n","\n","#     return copy_scale_score,r_values\n","######   ---------------risk_level_3 \n","import copy as c\n","import numpy as np\n","\n","def risk_level_3(in_data, scale_data,w_2_dir, d_info):\n","\n","    c_in_data = c.deepcopy(in_data)\n","    ci_99 = c_in_data.shape[1] - 1  # 99% CI column\n","    ci_99_val = c_in_data.iloc[:,ci_99]\n","    diff_idx=np.diff(ci_99_val)\n","    r_values=[]\n","    r_status=[] ## identification of the ci_99_val whether 0,1,2,3\n","    copy_scale_score = c.deepcopy(scale_data)\n","    risk_score_idx= copy_scale_score.shape[1]-1 #-----> added July 21/22 for DWT_PE\n","\t## check the difference is all the same  or not\n","    if (len(ci_99_val.unique()) ==1):\n","\t\t  \n","        if ci_99_val[0] == 0: ## no changes\n","            r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean())\n","            copy_scale_score[\"Level\"] = np.where(copy_scale_score[\"risk\"] < r_values, 'L', 'M')\n","            r_status=1\n","                #r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","                #r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","        elif  ci_99_val[0] == 1: ## decreasing\n","            r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean())\n","            copy_scale_score[\"Level\"] = np.where(copy_scale_score[\"risk\"] < r_values[0], 'M', 'H')\n","            r_status=2\n","                # r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","            #r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","        else: ## increasing\n","            copy_scale_score[\"Level\"] = 'H'\n","                # r_values = ((copy_scale_score.iloc[:,risk_score_idx]).mean()).tolist()\n","            r_values = [(copy_scale_score.iloc[:,risk_score_idx]).mean()]\n","            r_status=3\n","    else:  ## different values\n","\n","        r_status=0\n","        cut_off_value =  np.where(diff_idx != 0)\n","        for v in range(0, len(cut_off_value[0])):\n","          r_idx = int(c_in_data.iloc[cut_off_value[0][v]+1,0]) ## due to the diff calcualtion, the original index should be added by 1\n","          #print(r_idx)\n","          # get risk score values\n","          #temp_values = copy_scale_score.iloc[r_idx,copy_scale_score.shape[1]-1]\n","          temp_values = copy_scale_score.iloc[r_idx,risk_score_idx] #-----> modified July 21/22 for DWT_PE\n","          r_values.append(temp_values)\n","\n","          # assign risk level to data\n","          if len(cut_off_value[0]) == 1: # two level\n","            copy_scale_score[\"Level\"] = np.where(copy_scale_score[\"risk\"] < r_values[0], 'L', 'H')\n","          else: # three level\n","            min_val = min(r_values)\n","            max_val = max(r_values)\n","            copy_scale_score.loc[copy_scale_score[\"risk\"] < min_val, \"Level\"] = 'L'\n","            copy_scale_score.loc[(copy_scale_score['risk'] >= min_val) & (copy_scale_score['risk'] < max_val), \"Level\"] = 'M'\n","            copy_scale_score.loc[copy_scale_score['risk'] >= max_val, \"Level\"] = 'H'\n","\n","          l_file_name = file_name_create(d_info, '_risklevel')\n","          # l_name = str(day_info) + '_risklevel'\n","          # l_file_name = \"%s.csv\" % l_name\n","          copy_scale_score.to_csv(w_2_dir + '/' + l_file_name, index=False)\n","\n","    return copy_scale_score,r_values,r_status  \n","\n","######--------spearman_cor_test  ----------------------------\n","def spearman_cor_test(data):\n","    # calculate spearman's correlation\n","    correlation = pd.DataFrame()\n","    coef, p = spearmanr(data['risk'], data['KA'])\n","    coef1, p1 = spearmanr(data['risk'], data['Normal'])\n","    coef2, p2 = spearmanr(data['risk'], data['c_ses'])\n","    coef3, p3 = spearmanr(data['risk'], data['var'])\n","    r = [coef, coef1, coef2, coef3, p, p1, p2, p3]\n","    correlation = correlation.append(r, ignore_index=True)\n","    correlation = correlation.transpose()\n","    return correlation\n","\n","######----------file_name_create  --------------------------\n","def file_name_create(f_n, f_info):\n","    f_name = str(f_n) + f_info\n","    created_file_name = \"%s.csv\" % f_name\n","    return created_file_name\n","\n","\n","######-------------risk_score_generation_colab  -----------------------\n","def risk_score_generation_colab(fileter_df,filtered_scaled_data, current_data, day_info, w_2_dir,btrap_rep):\n","    spearman_cor = pd.DataFrame()\n","    df_scaled = c.deepcopy(filtered_scaled_data)\n","    # feature variance score\n","    col_len = filtered_scaled_data.shape[1]\n","    #----- data is MLE the, the column size is 41 --------------------------------\n","    # _scale_var = df_scaled.iloc[:, 0:37].var(axis=1)\n","    # actual_scaled_df = pd.concat([_scale_var, df_scaled.iloc[:, 37:col_len]], axis=1)\n","\n","     #----- data is DWT_PE the, the column size is 89 --------------------------------\n","    _scale_var = df_scaled.iloc[:, 0:85].var(axis=1)\n","    actual_scaled_df = pd.concat([_scale_var, df_scaled.iloc[:, 85:col_len]], axis=1) \n","    actual_scaled_df.columns = ['var', 'c_ses', 'Normal', 'KA', 'UA']\n","\n","    copy_df = c.deepcopy(fileter_df)\n","    scale_score = c.deepcopy(actual_scaled_df)\n","    col_list = ['KA', 'UA']\n","    scale_score['AttackSum'] = copy_df[col_list].sum(axis=1)\n","\n","\n","    scale_score['ratio_N'] = scale_score['Normal'] + scale_score['var']\n","    scale_score['ratio_A'] = (scale_score['UA'] + scale_score['KA']) + scale_score['var']\n","\n","    idx_n = scale_score.columns.get_loc(\"ratio_N\")\n","    idx_a = scale_score.columns.get_loc(\"ratio_A\")\n","    idx_s = fileter_df.columns.get_loc(\"c_ses\")\n","    #     def risk_score(data):\n","    scale_score['risk'] = (scale_score['ratio_A'] / (scale_score['ratio_N'] + scale_score['ratio_A'])) * fileter_df[\n","        'c_ses']\n","\n","\n","    print('Completed risk score--', str(day_info))\n","\n","    # spearman correlation between risk score and other varuables\n","    s = spearman_cor_test(scale_score)\n","\n","    spearman_cor = spearman_cor.append(s, ignore_index=True)\n","    #     # create a folder for month\n","\n","    print('Completed spearman --', str(day_info))\n","\n","    ### write to outfile\n","\n","    # os.chdir(current_folder) #change the month directory\n","    # print('Completed current folder --',current_folder)\n","    m_folder = (current_data.split('2015')[1][0:2])\n","\n","\n","    # # create a folder for month\n","    # if not os.path.exists(month_folder):\n","      \n","    #   os.makedirs(month_folder)\n","    # # join the month folder to existed directory       \n","    # write_dir = os.path.join(current_folder, month_folder)   \n","    # os.chdir(write_dir) #change the month directory\n","    # print('Completed write --',write_dir)\n","\n","    \n","       \n","    file_name = file_name_create(day_info, '_riskscore')\n","    cor_file_name = file_name_create(m_folder, 'Spearman_2_rs')\n","\n","\n","    print('Writing an output --', str(day_info))\n","\n","    \n","    scale_score.to_csv(file_name, index=False)\n","\n","    # if str(day_info).split('2015')[1][3:4] =='1':  ----------------->MLE\n","    if str(current_data).split('2015')[1][2:4] =='01':\n","      spearman_cor.columns = ['KA', 'N', 'Ses', 'var', 'p_KA', 'p_N', 'p_Ses', 'p_var']\n","      spearman_cor['day']=pd.Series(day_info)\n","      spearman_cor.to_csv(w_2_dir + '/' + cor_file_name, header=True, index=False)\n","    else:\n","      spearman_cor['day']=pd.Series(day_info)\n","      spearman_cor.to_csv(w_2_dir + '/' + cor_file_name, index=False, header=False, mode='a')\n","\n","   # spearman_cor.to_csv(w_2_dir + '/' + cor_file_name, index=True, mode='a')\n","    # scale_score.to_csv(write_dir + '/' + file_name, index=False)\n","    # spearman_cor.to_csv(write_dir + '/' + cor_file_name, index=False, mode='a')\n","\n","    print('Calcualting risk score for --', str(day_info))\n","    target = scale_score['risk']\n","    target_ecdf_values, y = ecdf_values(target)\n","\n","    print('Identifying peak change for --', str(day_info))\n","    change_values, change_idx = peak_change_V2(target_ecdf_values, str(day_info), save=False)\n","    #     idx_CI = bootstrap(change_idx_values, bootstrap_rep, func=np.mean)\n","    print('done for peak --', str(day_info))\n","    bootstrap_out_idx = bootstrap_out(change_values, change_idx, str(day_info), btrap_rep)\n","    print('done for bootstrap --', str(day_info))\n","\n","    level_out, risk_cut_off,r_status = risk_level_3(bootstrap_out_idx, scale_score, w_2_dir, day_info)\n","\n","    return level_out,risk_cut_off,r_status\n","\n","   \n","\n","######-------------ecdf_values  -----------------------\n","def ecdf_values(x):\n","    \"\"\"\n","    Generate values for empirical cumulative distribution function\n","    \n","    Params\n","    --------\n","        x (array or list of numeric values): distribution for ECDF\n","    \n","    Returns\n","    --------\n","        x (array): x values\n","        y (array): percentile values\n","    \"\"\"\n","    \n","    # Sort values and find length\n","    x = np.sort(x)\n","    n = len(x)\n","    # Create percentiles\n","    y = np.arange(1, n + 1, 1) / n\n","    return x, y\n","\n","######-------------peak_change_V2  -----------------------\n","def peak_change_V2(x, pig_name, save=False ):\n","    # fig = plt.figure(figsize=(10,6))\n","    f = plt.figure()\n","    # gs = gridspec.GridSpec(5, 1)\n","\n","    # k = np.gradient(a1_target)\n","    k = np.gradient(x)\n","    # ax0 = plt.subplot(gs[0])\n","    # ax0.plot(k)\n","\n","\n","    k2 = np.gradient(k)\n","    # ax1 = plt.subplot(gs[1])\n","    # ax1.plot(k2)\n","    # plt.savefig('2st derivative' + '.pdf')\n","    # plt.plot(k2)\n","    # f.savefig('2st derivativer' + '.pdf')\n","    # plt.close()\n","\n","    k3= np.clip(np.abs(np.gradient(k2)), 0.0001, 2)\n","    # ax3 = plt.subplot(gs[3])\n","    # ax3.plot(k3)\n","    # plt.plot(k3)\n","    # # plt.savefig('absolute value ' + '.pdf')\n","    # f.savefig('absolute valuer ' + '.pdf')\n","    # plt.close()\n","\n","    smoothed_k = gaussian_filter1d(k3, 20)\n","    # ax4 = plt.subplot(gs[4])\n","    # ax4.plot(smoothed_k)\n","    # plt.savefig('Smoothing applied' + '.pdf')\n","    # plt.plot(smoothed_k)\n","    # f.savefig('Smoothing appliedr' + '.pdf')\n","    # plt.close()\n","\n","    max_idx = argrelmax(smoothed_k)[0]\n","    # print(max_idx)\n","\n","\n","### when plot\n","\n","    # if save:\n","    #     plt.plot(k)\n","    #     plt_name = pig_name\n","    #     f.savefig(pig_name + '.pdf')\n","\n","    #     plt.plot(k2)\n","    #     f.savefig(pig_name + '2stDeri.pdf')\n","\n","    #     plt.plot(k3)\n","    #     # plt.savefig('absolute value ' + '.pdf')\n","    #     f.savefig(pig_name + 'absolute.pdf')\n","\n","    #     plt.plot(smoothed_k)\n","    #     f.savefig(pig_name +'Smoothing.pdf')\n","\n","    #     fig, ax = plt.subplots()\n","    #     ax.set_title('Risk score evaluation')\n","    #     ax.set_xlabel('time')\n","    #     ax.set_ylabel('Risk score evaluation')\n","    #     ax.plot(x)\n","    #     ax.scatter(max_idx, x[max_idx], marker='o', color='red')\n","    #     plt.show()\n","    #     # plt.savefig('slope change mark' + '.png')\n","    #     f.savefig(pig_name +'slope_change_mark.pdf')\n","\n","    #     plt.close()\n","\n","    return x[max_idx], max_idx\n","\n","######-------------bootstrap  -----------------------\n","def bootstrap(b_data, n_rep, func=np.mean):\n","\n","    \"\"\"\n","    Generate `n` bootstrap samples, evaluating `func`\n","    at each resampling. `bootstrap` returns a function,\n","    which can be called to obtain confidence intervals\n","    of interest.\n","    \"\"\"\n","    simulations = list()\n","    sample_size = len(b_data)\n","    #     xbar_init = np.mean(data)\n","    xbar_init = np.var(b_data)\n","    for c in range(n_rep):\n","        itersample = np.random.choice(b_data, size=sample_size, replace=True)\n","        simulations.append(func(itersample))\n","    simulations.sort()\n","\n","    def ci(p):\n","        \"\"\"\n","        Return 2-sided symmetric confidence interval specified\n","        by p.\n","        \"\"\"\n","        u_pval = (1 + p) / 2.\n","        l_pval = (1 - u_pval)\n","        l_indx = int(np.floor(n_rep * l_pval))\n","        u_indx = int(np.floor(n_rep * u_pval))\n","        return (simulations[l_indx], simulations[u_indx])\n","\n","    return (ci)\n","\n","######-------------bootstrap_out  -----------------------\n","def bootstrap_out(c_data,c_idx,f_name, rep):\n","    total_value = []\n","    total_value2 = []\n","    total_value3 = []\n","\n","    # copy_data = c.deepcopy(x[max_idx])\n","    sample = c_data[0]\n","\n","    for i in range(0, c_data.shape[0]):\n","        sample = np.append(sample, c_data[i])\n","\t\t\n","        var_sample = sample.var()\n","        #     var_sample = s.mean(sample)\n","        boot = bootstrap(sample, rep)\n","\t\t# len(cintervals)= 3\n","        cintervals = [boot(j) for j in (.90, .95, .99)]\n","\n","        #         cintervals = [boot(i) for i in range(len(ci_range)-1) ]\n","        k = 0\n","        while k <= len(cintervals) - 1:\n","            # check whther a var is in the CI\n","            if (var_sample >= cintervals[k][0]) & (var_sample <= cintervals[k][1]):\n","                out = 1  ### print('Y')\n","            else:\n","                out = 0\n","\n","            if k == 0:\n","                out_value = [i, var_sample, cintervals[k][0], cintervals[k][1], out]\n","                total_value.append(out_value)\n","            elif k == 1:\n","                out_value2 = [cintervals[k][0], cintervals[k][1], out]\n","                total_value2.append(out_value2)\n","            else:\n","                out_value3 = [cintervals[k][0], cintervals[k][1], out]\n","                total_value3.append(out_value3)\n","\n","            k = k + 1\n","    #     out_value = [i,cintervals[0][0],cintervals[0][1],cintervals[1][0],cintervals[1][1],cintervals[2][0],cintervals[2][1]]\n","\n","    #     out_value = [i,var_sample,ci_sample[0],ci_sample[1],out]\n","    total_value = pd.DataFrame(total_value)\n","    total_value2 = pd.DataFrame(total_value2)\n","    total_value3 = pd.DataFrame(total_value3)\n","    cg_idx = pd.DataFrame(c_idx[0:]) # index for change point --> modified july 26 from 1 to 0\n","\n","    all_CIs = pd.concat([cg_idx, total_value, total_value2, total_value3], axis=1)\n","    all_CIs.columns  = ['id', 'idx', 'var','.9Low', '.9up', 'out_9', '.95L', '.95U', 'out_95', '.99L', '.99U', 'out_99']\n","    out_name= f_name + '_' + str(rep) + '_bootstrapC'\n","    out_file_name = \"%s.csv\" % out_name\n","    # all_CIs=pd.DataFrame(all_CIs,columns=['idx','.9Low','.9up','out_9','.95L','.95U','out_95','.99L','.99U','out_99',])\n","    all_CIs.to_csv(out_file_name, index=False)\n","    # all_CIs.to_csv('bootstrapVar5000_CI_ratio.csv', index=False)\n","\n","    return all_CIs"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"AttackRisk_DWTPE-Aug20-22.ipynb","provenance":[],"background_execution":"on","authorship_tag":"ABX9TyPVZ3aoB3ceb4fA1raCzjYj"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}