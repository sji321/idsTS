{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "risk_github_May.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6Pq+Lt+PN7AWjwxkvok26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sji321/kyoto/blob/main/code/risk_github_May.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB4ItfXoqs2S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyreadr"
      ],
      "metadata": {
        "id": "GR9Xo7muqvD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fs = 60  # training\n",
        "hours_ = 16  # hours info\n",
        "bootstrap_rep =2000\n",
        "\n",
        "data_folder=\"/content/drive/MyDrive/Kyoto/data_01\"\n",
        "# data_folder = \"F:\\\\KyotoTemporal\\\\out\\\\MLE\\\\test\\\\\"\n",
        "# data_folder=\"F:\\\\KyotoTemporal\\\\out\\\\MLE_TS_1\\\\01\\\\\"\n",
        "# out_folder = \"F:\\\\KyotoTemporal\\\\out\\\\RNN_out\\\\RiskEstimate\\\\\"\n",
        "out_folder = \"/content/drive/MyDrive/Kyoto\"\n",
        "## Monthly data store\n",
        "# base_folder = \"F:\\\\KyotoTemporal\\\\out\\\\RNN_out\\\\Fcast_Actual\\\\\"\n",
        "\n",
        "day_list = []\n",
        "df3=pd.DataFrame()\n",
        "\n",
        "\n",
        "for folder in os.listdir(data_folder):\n",
        "    d = os.path.join('/content/drive/MyDrive/Kyoto/data_01', folder)\n",
        "    #     df =pd.read_csv(d)\n",
        "    result = pyreadr.read_r(d)\n",
        "\n",
        "    print(result.keys())\n",
        "    input_data = result[\"user_ts\"]  # extract the pandas data frame for object c6\n",
        "\n",
        "    d1 = input_data.drop([\"i\"], axis=1)\n",
        "\n",
        "    day = pd.to_numeric((d.split('1_')[1]).split('.')[0])\n",
        "    \n",
        "    day_list.append(day)\n",
        "    out_perm = np.array([])\n",
        "  \n",
        "    np_filter_d1 = d1.fillna(method='bfill')  # ffill - forward-fill propagate inplace=False\n",
        "\n",
        "    # for TESTING\n",
        "    # filtered_d1 = c.deepcopy(np_filter_d1.iloc[0:7199,:])\n",
        "    # hours_=1\n",
        "    \n",
        "    # ------ Entire data\n",
        "    filtered_d1 = c.deepcopy(np_filter_d1)\n",
        "    \n",
        "    \n",
        "#     train_size=int(filtered_d1.shape[0]*0.7)\n",
        "# 16 hours of data for training\n",
        "    train_size=int(fs*hours_*60)\n",
        "    col_len = filtered_d1.shape[1]\n",
        "    \n",
        "    # seperate training and testing data\n",
        "    tr_df = c.deepcopy(filtered_d1[0:train_size])\n",
        "    tst_df = c.deepcopy(filtered_d1[train_size:])\n",
        "    \n",
        "# feature varianve ----------------------------------------------------\n",
        "\t # all actual variable\n",
        "    combine_actual_df =pd.concat([tr_df, tst_df], axis=0)\n",
        "\t# original actal feature variance\n",
        "    actual_df_var = combine_actual_df.iloc[:,0:37].var(axis=1)\n",
        "\t# integration of actual feature variance and class such as session, normal, KA, UA\n",
        "    actual_df = pd.concat([actual_df_var, combine_actual_df.iloc[:,37:col_len]], axis=1)\n",
        "    \n",
        "    # scaling to (0,1)\n",
        "    train_df = c.deepcopy(tr_df)\n",
        "    scalers={}\n",
        "    for i in tr_df.columns:\n",
        "        scaler = MinMaxScaler(feature_range=(0,1))\n",
        "        s_s = scaler.fit_transform(train_df[i].values.reshape(-1,1))\n",
        "        s_s=np.reshape(s_s,len(s_s))\n",
        "        scalers['scaler_'+ i] = scaler\n",
        "        train_df[i]=s_s\n",
        "    test_df = c.deepcopy(tst_df)  \n",
        "    for i in tr_df.columns:\n",
        "        scaler = scalers['scaler_'+i]\n",
        "        s_s = scaler.transform(test_df[i].values.reshape(-1,1))\n",
        "        s_s=np.reshape(s_s,len(s_s))\n",
        "        scalers['scaler_'+i] = scaler\n",
        "        test_df[i]=s_s\n",
        "\n",
        "       # train and test data combine\n",
        "    data_scaled = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "\t# feature variance for scaled data \n",
        "  #   col_len = filtered_d1.shape[1]\n",
        "  #   _scale_var = data_scaled.iloc[:,0:37].var(axis=1)\n",
        "\t# # integration of scaled feature variance and class such as session, normal, KA, UA\n",
        "  #   actual_scaled_df = pd.concat([_scale_var, data_scaled.iloc[:,37:col_len]], axis=1)\n",
        "  #   actual_scaled_df.columns = ['var', 'c_ses', 'Normal', 'KA', 'UA']\n",
        "\t#del filtered_d1, tr_df,tst_df,combine_actual_df,actual_df_var,_scale_var   \n",
        "\n",
        "# prepare for writing output -------------------------------\n",
        "    os.chdir(out_folder) #change the month directory\n",
        "    # print('Completed current folder --',out_folder)\n",
        "    month_folder = (d.split('2015')[1][0:2])\n",
        "\n",
        "\n",
        "    # create a folder for month\n",
        "    if not os.path.exists(month_folder):\n",
        "      \n",
        "      os.makedirs(month_folder)\n",
        "    # join the month folder to existed directory       \n",
        "    write_dir = os.path.join(out_folder, month_folder)   \n",
        "    os.chdir(write_dir) #change the month directory\n",
        "    print('Completed write --',write_dir)\n",
        "\n",
        "    # risk level and threshold\n",
        "    level, t = risk_score_generation_colab(filtered_d1,data_scaled, d, day, write_dir,bootstrap_rep)\n",
        "    \n",
        "\n",
        "      \n",
        "    \n",
        "#     # dividing traing and testing with defined train size\n",
        "#     train_df,test_df = data_scaled[0:train_size], data_scaled[train_size:] \n",
        "# class info to be forecasted \n",
        "    out_idx = [d1.columns.get_loc(\"c_ses\"), d1.columns.get_loc(\"Normal\"), d1.columns.get_loc(\"KA\"),d1.columns.get_loc(\"UA\")]\n",
        "    \n",
        "     # Set the input_sequence_length length - this is the timeframe used to make a single prediction\n",
        "    input_sequence_length = fs # number of features\n",
        "\n",
        "     # output_sequence_length = len(out_idx) # number of outputs\n",
        "    output_sequence_length = 1 # number of outputs\n",
        "\n",
        "# spliting data to features and class\n",
        "    x_train, y_train = partition_dataset(input_sequence_length, output_sequence_length, train_df.values,out_idx)\n",
        "    x_test, y_test = partition_dataset(input_sequence_length, output_sequence_length, test_df.values,out_idx)\n",
        "\n",
        "# LSTM model generation and training\n",
        "    model = Sequential()\n",
        "    # n_output_neurons = output_sequence_length\n",
        "    n_output_neurons = 4\n",
        "\n",
        "    n_input_neurons = x_train.shape[1] * x_train.shape[2]\n",
        "#     n_input_neurons = x_train.shape[2]\n",
        "    print(n_input_neurons, x_train.shape[1], x_train.shape[2])\n",
        "    model.add(LSTM(n_input_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(LSTM(int(n_input_neurons/2), return_sequences=False))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # model.add(Dense(output_sequence_length))\n",
        "    model.add(Dense(n_output_neurons))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy']) \n",
        "    model.summary()\n",
        "    # Training the model\n",
        "\n",
        "    epochs = 1\n",
        "    batch_size = 2\n",
        "    early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "    history = model.fit(x_train, y_train,batch_size=batch_size, \n",
        "                        epochs=epochs, validation_data=(x_test, y_test))\n",
        "    \n",
        "    # predict\n",
        "    pred_e1d1=model.predict(x_test)\n",
        "\n",
        "    y_pred = scaler.inverse_transform(pred_e1d1)\n",
        "    y_pred= y_pred.reshape((len(y_test), n_output_neurons))\n",
        "    #reshape_test= y_test.reshape((len(y_test), n_output_neurons))\n",
        "    #inv_y_test = scaler.inverse_transform(reshape_test) #---- not needed\n",
        "    # inverse transform for testing label\n",
        "    # inv_test= scaler.inverse_transform(y_test.reshape(-1,1)).reshape(y_test.shape) \n",
        "    inv_y_test= y_test.reshape((len(y_test), n_output_neurons))\n",
        "\n",
        "    # # actual risk level from data scale     \n",
        "    # act_d_r_l = dy_level_risk[train_size:] # for actual risk level\n",
        "    \n",
        "\n",
        "   \n",
        "    # tst_risk_score = risk_score_calculation(test_df,tst_act_scaled_df,d,out_folder)\n",
        "\n",
        "\n",
        "     ##  RMSE accuracy measures\n",
        "    \n",
        "    for i in range(0,n_output_neurons):\n",
        "        #when target attributes are also scaled\n",
        "        p= mean_squared_error(inv_y_test[:,i], y_pred[:,i], squared=False)\n",
        "        mse= mean_squared_error(inv_y_test[:,i], y_pred[:,i])\n",
        "        mae = mean_squared_error(inv_y_test[:,i], y_pred[:,i])\n",
        "        # when target attributes are not scaled\n",
        "\n",
        "        out_perm = np.hstack((out_perm, p,mae, mse)) \n",
        "#         \n",
        "        del p,mae,mse\n",
        "    \n",
        "    out_perm = np.hstack((day,out_perm))\n",
        "    \n",
        "    \n",
        "    out=pd.DataFrame(out_perm)\n",
        "    out =out.T\n",
        "    out_perm=pd.DataFrame(out_perm)\n",
        "    y_pred=pd.DataFrame(y_pred)\n",
        "    inv_y_test=pd.DataFrame(inv_y_test)\n",
        "\n",
        "    df3 = pd.concat([df3,out], axis=0)\n",
        "\n",
        "    tst_row_dim = (test_df.shape)[0]\n",
        "    tst_scale_var=test_df.iloc[fs+1:tst_row_dim,0:37].var(axis=1)\n",
        "\n",
        "    pd.DataFrame(tst_scale_var)\n",
        "    tst_scale_var.reset_index(drop=True,inplace=True)\n",
        "    \n",
        "    pred_row_var = pd.concat([tst_scale_var,y_pred], axis=1)\n",
        "    inv_row_var = pd.concat([tst_scale_var,inv_y_test], axis=1)\n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "# write to output\n",
        "    day_folder = (d.split('2015')[1][3:4])\n",
        "    hdr=['fea_var','session','N','A','NA']\n",
        "    fcast_name = file_name_create(day_folder, '_Fcast_values')\n",
        "    pred_row_var.to_csv(write_dir + '/' + fcast_name, index=False,header=hdr) \n",
        "\n",
        "    acual_name = file_name_create(day_folder, '_Actual_values')\n",
        "    inv_row_var.to_csv(write_dir + '/' + acual_name, index=False, header=hdr)  \n",
        "\n",
        "    # target = scale_score['risk']\n",
        "    # target_ecdf_values, y = ecdf_values(target)\n",
        "    # change_values, change_idx = peak_change_V2(target_ecdf_values, str(day), save=False)\n",
        "    # bootstrap_out_idx = bootstrap_out(change_values, change_idx, str(day), btrap_rep)\n",
        "    \n",
        "    # tst_level, t = risk_score_generation_colab(filtered_d1,data_scaled, write_dir, d, day, 'Risk',bootstrap_rep)\n",
        "    tst_act_depnedvarOnly= test_df.iloc[0:(y_pred.shape)[0],37:(test_df.shape)[1]]\n",
        "    tst_act_depnedvarOnly.reset_index(drop=True,inplace=True)\n",
        "    tst_scale_var = test_df.iloc[:,0:37].var(axis=1)\n",
        "     # # integration of scaled feature variance and class such as session, normal, KA, UA for test data\n",
        "    tst_act_scaled_df = pd.concat([tst_scale_var, test_df.iloc[:,37:col_len]], axis=1)\n",
        "    tst_act_scaled_df.reset_index(drop=True,inplace=True)\n",
        "    tst_act_scaled_df.columns = ['var', 'c_ses', 'Normal', 'KA', 'UA']\n",
        "    \n",
        "\n",
        "\n",
        "    test_score = fcast_risk_level_colab (tst_act_depnedvarOnly,tst_scaled_var,day, write_dir)\n",
        "    # confusion matrix of risk levels between test depenent variabls and the forecasted variables\n",
        "    Risklevel_performance (level, train_size, test_score, y_pred, day, write_dir)\n",
        "\n",
        "if (d.split('2015')[1][3:4]) == '1' :\n",
        "    df3.columns = ['Day','S_RMSE','S_MAE','S_MSE','N_RMSE','N_MAE','N_MSE','A_RMSE','A_MAE','A_MSE','UA_RMSE','UA_MAE','UA_MSE']\n",
        "    # fcast_perm_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n",
        "    fcast_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n",
        "    df3.to_csv(write_dir + '/' + fcast_name,  index=False) \n",
        "      \n",
        "else:\n",
        "    fcast_name = file_name_create(month_folder, '_RMSE_MAPE_LSTM')\n",
        "    # df3.to_csv(write_dir + '/' + fcast_perm_name,  index=False, mode='a',header=False)\n",
        "    df3.to_csv(write_dir + '/' + fcast_name,  index=False, mode='a',header=False)\n"
      ],
      "metadata": {
        "id": "ykl3w4Bkqyc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyreadr\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from numpy import array\n",
        "from numpy.random import uniform\n",
        "from numpy import hstack\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math as m\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import copy as c\n",
        "from  google.colab import drive\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "from scipy.signal import argrelmax\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "def fcast_risk_level_colab (tst_act_depvals,tst_scaled_var,cut_threshold, day, write_dir):\n",
        "\n",
        "\n",
        "  copy_tdf = c.deepcopy(tst_act_depvals)\n",
        "  scale_tscore = c.deepcopy(tst_scaled_var)\n",
        "\n",
        "  col_list = ['KA', 'UA']\n",
        "  scale_tscore['AttackSum'] = copy_tdf[col_list].sum(axis=1)\n",
        "\n",
        "  scale_tscore['ratio_N'] = scale_tscore['Normal'] + scale_tscore['var']\n",
        "  scale_tscore['ratio_A'] = (scale_tscore['UA'] + scale_tscore['KA']) + scale_tscore['var']\n",
        "\n",
        "  idx_n = scale_tscore.columns.get_loc(\"ratio_N\")\n",
        "  idx_a = scale_tscore.columns.get_loc(\"ratio_A\")\n",
        "  idx_s = tst_act_depvals.columns.get_loc(\"c_ses\")\n",
        "  #     def risk_score(data):\n",
        "  scale_tscore['risk'] = (scale_tscore['ratio_A'] / (scale_tscore['ratio_N'] + scale_tscore['ratio_A'])) * copy_tdf[\n",
        "      'c_ses']\n",
        "\n",
        "  if len(cut_threshold) == 1: # two level\n",
        "        scale_tscore[\"level\"] = np.where(scale_tscore[\"risk\"] < t2[0], 'low', 'High')\n",
        "  # elif len(cut_off_value[0]) == 0: \n",
        "  #   ...\n",
        "  else: # three level\n",
        "        min_val = min(cut_threshold)\n",
        "        max_val = max(cut_threshold)\n",
        "        scale_tscore.loc[scale_tscore[\"risk\"] < min_val, \"Level\"] = 'L'\n",
        "        scale_tscore.loc[(scale_tscore['risk'] >= min_val) & (scale_tscore['risk'] < max_val), \"Level\"] = 'M'\n",
        "        scale_tscore.loc[scale_tscore['risk'] >= max_val, \"Level\"] = 'H'\n",
        "\n",
        "  _name = file_name_create(day, '_fcast_risk_level')\n",
        "  scale_tscore.to_csv(write_dir + '/' + _name,  index=False)\n",
        "\n",
        "  return scale_tscore\n",
        "\n",
        "\n",
        "def Risklevel_performance (all_level, tr_size, scale_tscore, yhat, day, write_dir):\n",
        "\t\n",
        "    all_risk_level = c.deepcopy(all_level)\n",
        "    yhat_df = c.deepcopy(yhat)\n",
        "    cm_risk=pd.DataFrame()\n",
        "\n",
        "\n",
        "    df_level = all_risk_level[tr_size:]\n",
        "    df_level.reset_index(drop=True,inplace=True)\n",
        "    tst_level=df_level.iloc[0:(yhat_df.shape)[0],:]\n",
        "\n",
        "    len_tst_risk = len(pd.unique(tst_level['Level']))\n",
        "\n",
        "    if len_tst_risk==2:\n",
        "\n",
        "      tn, fp, fn, tp = confusion_matrix(tst_level['Level'], scale_tscore['Level']).ravel()\n",
        "      total=(tn+fp+fn+tp)\n",
        "      acc = (tp+tn)/total\n",
        "      sen =  tp/(tp+fn)\n",
        "      spec = tn/(fp+tn)\n",
        "    else:\n",
        "      multilabel_confusion_matrix(tst_level['Level'], scale_tscore['Level'])\n",
        "\n",
        "\n",
        "    cm = np.hstack((day,acc,sen,spec))\n",
        "    # cm1 =np.transpose(cm)\n",
        "    cm1=pd.DataFrame(cm)\n",
        "\n",
        "    cm_name = file_name_create(day, '_cm_risk_level')\n",
        "    cm1.to_csv(write_dir + '/' + cm_name, index=False)\n",
        "\n",
        "\n",
        "\n",
        "def split_series(series, n_past, n_future):\n",
        "\n",
        "  #\n",
        "  # n_past ==> no of past observations\n",
        "  #\n",
        "  # n_future ==> no of future observations \n",
        "  #\n",
        "    X, y = list(), list()\n",
        "    for window_start in range(len(series)):\n",
        "        past_end = window_start + n_past\n",
        "        future_end = past_end + n_future\n",
        "        if future_end > len(series):\n",
        "            break\n",
        "    # slicing the past and future parts of the window\n",
        "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
        "        X.append(past)\n",
        "        y.append(future)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def partition_dataset(input_sequence_length, output_sequence_length, data,index_Close):\n",
        "    x, y = [], []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(input_sequence_length, data_len - output_sequence_length):\n",
        "        x.append(data[i-input_sequence_length:i,0:index_Close[0]]) #contains input_sequence_length values 0-input_sequence_length * columns\n",
        "        y.append(data[i:i + output_sequence_length, index_Close]) #contains the prediction values for validation (3rd column = Close),  for single-step prediction\n",
        "    \n",
        "    # Convert the x and y to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# def mape(actual, pred): \n",
        "#     actual, pred = np.array(actual), np.array(pred)\n",
        "#     return np.mean(np.abs((actual - pred) / actual)) * 100\n",
        "\n",
        "def percentage_error(actual, predicted):\n",
        "    res = np.empty(actual.shape)\n",
        "    for j in range(actual.shape[0]):\n",
        "        if actual[j] != 0:\n",
        "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
        "        else:\n",
        "            res[j] = predicted[j] / np.mean(actual)\n",
        "    return resp\n",
        "\n",
        "# def mean_absolute_percentage_error(y_true, y_pred): \n",
        "# #     return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
        "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "#     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "#     return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "\n",
        "def fcast_acc_measure(actual, pred):\n",
        "    fcast_acc=[]\n",
        "    len_pred=pred_e1d1.shape[1]\n",
        "    i=0\n",
        "    while i< len_pred:\n",
        "        mae = mean_absolute_error(actual[:,i], pred[:,i])\n",
        "        rmse = m.sqrt(mean_squared_error(actual[:,i], pred[:,i]))\n",
        "        mape= mean_absolute_percentage_error(actual[:,i], pred[:,i])\n",
        "        \n",
        "        p=([mae,rmse,mape])\n",
        "        fcast_acc=np.concatenate([fcast_acc,p])\n",
        "        i += 1\n",
        "    return p\n",
        "\n",
        "import copy as c\n",
        "import numpy as np\n",
        "# import file_name_create as fc\n",
        "\n",
        "def risk_level(in_data, scale_data,w_2_dir, d_info):\n",
        "    c_in_data = c.deepcopy(in_data)\n",
        "    ci_99 = c_in_data.shape[1] - 1  # 99% CI column\n",
        "    t = c_in_data.iloc[:,ci_99]\n",
        "    diff_idx=np.diff(t)\n",
        "    r_values=[]\n",
        "\n",
        "    copy_scale_score = c.deepcopy(scale_data)\n",
        "    cut_off_value =  np.where(diff_idx != 0)\n",
        "    for v in range(0, len(cut_off_value[0])):\n",
        "        r_idx = int(c_in_data.iloc[cut_off_value[0][v]+1,0]) \n",
        "        #print(r_idx)\n",
        "        # get risk score values\n",
        "        temp_values = copy_scale_score.iloc[r_idx,copy_scale_score.shape[1]-1]\n",
        "        r_values.append(temp_values)\n",
        "\n",
        "        # assign risk level to data\n",
        "    if len(cut_off_value[0]) == 1: # two level\n",
        "        copy_scale_score[\"level\"] = np.where(copy_scale_score[\"risk\"] < r_values[0], 'low', 'High')\n",
        "    else: # three level\n",
        "        min_val = min(r_values)\n",
        "        max_val = max(r_values)\n",
        "        copy_scale_score.loc[copy_scale_score[\"risk\"] < min_val, \"Level\"] = 'L'\n",
        "        copy_scale_score.loc[(copy_scale_score['risk'] >= min_val) & (copy_scale_score['risk'] < max_val), \"Level\"] = 'M'\n",
        "        copy_scale_score.loc[copy_scale_score['risk'] >= max_val, \"Level\"] = 'H'\n",
        "\n",
        "    l_file_name = file_name_create(d_info, '_risklevel')\n",
        "    # l_name = str(day_info) + '_risklevel'\n",
        "    # l_file_name = \"%s.csv\" % l_name\n",
        "    copy_scale_score.to_csv(w_2_dir + '/' + l_file_name, index=False)\n",
        "\n",
        "    return copy_scale_score,r_values\n",
        "\n",
        "def spearman_cor_test(data):\n",
        "    # calculate spearman's correlation\n",
        "    correlation = pd.DataFrame()\n",
        "    coef, p = spearmanr(data['risk'], data['KA'])\n",
        "    coef1, p1 = spearmanr(data['risk'], data['Normal'])\n",
        "    coef2, p2 = spearmanr(data['risk'], data['c_ses'])\n",
        "    coef3, p3 = spearmanr(data['risk'], data['var'])\n",
        "    r = [coef, coef1, coef2, coef3, p, p1, p2, p3]\n",
        "    correlation = correlation.append(r, ignore_index=True)\n",
        "    correlation = correlation.transpose()\n",
        "    return correlation\n",
        "\n",
        "def file_name_create(f_n, f_info):\n",
        "    f_name = str(f_n) + f_info\n",
        "    created_file_name = \"%s.csv\" % f_name\n",
        "    return created_file_name\n",
        "\n",
        "\n",
        "def risk_score_generation_colab(fileter_df,filtered_scaled_data, current_data, day_info, w_2_dir,btrap_rep):\n",
        "    spearman_cor = pd.DataFrame()\n",
        "    df_scaled = c.deepcopy(filtered_scaled_data)\n",
        "    # feature variance score\n",
        "    col_len = filtered_scaled_data.shape[1]\n",
        "    _scale_var = df_scaled.iloc[:, 0:37].var(axis=1)\n",
        "    actual_scaled_df = pd.concat([_scale_var, df_scaled.iloc[:, 37:col_len]], axis=1)\n",
        "    actual_scaled_df.columns = ['var', 'c_ses', 'Normal', 'KA', 'UA']\n",
        "\n",
        "    copy_df = c.deepcopy(fileter_df)\n",
        "    scale_score = c.deepcopy(actual_scaled_df)\n",
        "    col_list = ['KA', 'UA']\n",
        "    scale_score['AttackSum'] = copy_df[col_list].sum(axis=1)\n",
        "\n",
        "\n",
        "    scale_score['ratio_N'] = scale_score['Normal'] + scale_score['var']\n",
        "    scale_score['ratio_A'] = (scale_score['UA'] + scale_score['KA']) + scale_score['var']\n",
        "\n",
        "    idx_n = scale_score.columns.get_loc(\"ratio_N\")\n",
        "    idx_a = scale_score.columns.get_loc(\"ratio_A\")\n",
        "    idx_s = fileter_df.columns.get_loc(\"c_ses\")\n",
        "    #     def risk_score(data):\n",
        "    scale_score['risk'] = (scale_score['ratio_A'] / (scale_score['ratio_N'] + scale_score['ratio_A'])) * fileter_df[\n",
        "        'c_ses']\n",
        "\n",
        "\n",
        "    print('Completed risk score--', str(day_info))\n",
        "\n",
        "    # spearman correlation between risk score and other varuables\n",
        "    s = spearman_cor_test(scale_score)\n",
        "\n",
        "    spearman_cor = spearman_cor.append(s, ignore_index=True)\n",
        "    #     # create a folder for month\n",
        "\n",
        "    print('Completed spearman --', str(day_info))\n",
        "\n",
        "    ### write to outfile\n",
        "\n",
        "    # os.chdir(current_folder) #change the month directory\n",
        "    # print('Completed current folder --',current_folder)\n",
        "    m_folder = (current_data.split('2015')[1][0:2])\n",
        "\n",
        "\n",
        "    # # create a folder for month\n",
        "    # if not os.path.exists(month_folder):\n",
        "      \n",
        "    #   os.makedirs(month_folder)\n",
        "    # # join the month folder to existed directory       \n",
        "    # write_dir = os.path.join(current_folder, month_folder)   \n",
        "    # os.chdir(write_dir) #change the month directory\n",
        "    # print('Completed write --',write_dir)\n",
        "\n",
        "    \n",
        "       \n",
        "    file_name = file_name_create(day_info, '_riskscore')\n",
        "    cor_file_name = file_name_create(m_folder, 'Spearman_2_rs')\n",
        "\n",
        "\n",
        "    print('Writing an output --', str(day_info))\n",
        "\n",
        "    spearman_cor.columns = ['KA', 'N', 'Ses', 'var', 'p_KA', 'p_N', 'p_Ses', 'p_var']\n",
        "    scale_score.to_csv(file_name, index=False)\n",
        "    spearman_cor.to_csv(w_2_dir + '/' + cor_file_name, index=False, mode='a')\n",
        "    # scale_score.to_csv(write_dir + '/' + file_name, index=False)\n",
        "    # spearman_cor.to_csv(write_dir + '/' + cor_file_name, index=False, mode='a')\n",
        "\n",
        "    print('Calcualting risk score for --', str(day_info))\n",
        "    target = scale_score['risk']\n",
        "    target_ecdf_values, y = ecdf_values(target)\n",
        "\n",
        "    print('Identifying peak change for --', str(day_info))\n",
        "    change_values, change_idx = peak_change_V2(target_ecdf_values, str(day_info), save=False)\n",
        "    #     idx_CI = bootstrap(change_idx_values, bootstrap_rep, func=np.mean)\n",
        "    print('done for peak --', str(day_info))\n",
        "    bootstrap_out_idx = bootstrap_out(change_values, change_idx, str(day_info), btrap_rep)\n",
        "    print('done for bootstrap --', str(day_info))\n",
        "\n",
        "    level_out, risk_cut_off = risk_level(bootstrap_out_idx, scale_score, w_2_dir, day_info)\n",
        "\n",
        "    return level_out,risk_cut_off\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "def ecdf_values(x):\n",
        "    \"\"\"\n",
        "    Generate values for empirical cumulative distribution function\n",
        "    \n",
        "    Params\n",
        "    --------\n",
        "        x (array or list of numeric values): distribution for ECDF\n",
        "    \n",
        "    Returns\n",
        "    --------\n",
        "        x (array): x values\n",
        "        y (array): percentile values\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sort values and find length\n",
        "    x = np.sort(x)\n",
        "    n = len(x)\n",
        "    # Create percentiles\n",
        "    y = np.arange(1, n + 1, 1) / n\n",
        "    return x, y\n",
        "\n",
        "def peak_change_V2(x, pig_name, save=False ):\n",
        "    # fig = plt.figure(figsize=(10,6))\n",
        "    f = plt.figure()\n",
        "    # gs = gridspec.GridSpec(5, 1)\n",
        "\n",
        "    # k = np.gradient(a1_target)\n",
        "    k = np.gradient(x)\n",
        "    # ax0 = plt.subplot(gs[0])\n",
        "    # ax0.plot(k)\n",
        "\n",
        "\n",
        "    k2 = np.gradient(k)\n",
        "    # ax1 = plt.subplot(gs[1])\n",
        "    # ax1.plot(k2)\n",
        "    # plt.savefig('2st derivative' + '.pdf')\n",
        "    # plt.plot(k2)\n",
        "    # f.savefig('2st derivativer' + '.pdf')\n",
        "    # plt.close()\n",
        "\n",
        "    k3= np.clip(np.abs(np.gradient(k2)), 0.0001, 2)\n",
        "    # ax3 = plt.subplot(gs[3])\n",
        "    # ax3.plot(k3)\n",
        "    # plt.plot(k3)\n",
        "    # # plt.savefig('absolute value ' + '.pdf')\n",
        "    # f.savefig('absolute valuer ' + '.pdf')\n",
        "    # plt.close()\n",
        "\n",
        "    smoothed_k = gaussian_filter1d(k3, 20)\n",
        "    # ax4 = plt.subplot(gs[4])\n",
        "    # ax4.plot(smoothed_k)\n",
        "    # plt.savefig('Smoothing applied' + '.pdf')\n",
        "    # plt.plot(smoothed_k)\n",
        "    # f.savefig('Smoothing appliedr' + '.pdf')\n",
        "    # plt.close()\n",
        "\n",
        "    max_idx = argrelmax(smoothed_k)[0]\n",
        "    # print(max_idx)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if save:\n",
        "        plt.plot(k)\n",
        "        plt_name = pig_name\n",
        "        f.savefig(pig_name + '.pdf')\n",
        "\n",
        "        plt.plot(k2)\n",
        "        f.savefig(pig_name + '2stDeri.pdf')\n",
        "\n",
        "        plt.plot(k3)\n",
        "        # plt.savefig('absolute value ' + '.pdf')\n",
        "        f.savefig(pig_name + 'absolute.pdf')\n",
        "\n",
        "        plt.plot(smoothed_k)\n",
        "        f.savefig(pig_name +'Smoothing.pdf')\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.set_title('Risk score evaluation')\n",
        "        ax.set_xlabel('time')\n",
        "        ax.set_ylabel('Risk score evaluation')\n",
        "        ax.plot(x)\n",
        "        ax.scatter(max_idx, x[max_idx], marker='o', color='red')\n",
        "        plt.show()\n",
        "        # plt.savefig('slope change mark' + '.png')\n",
        "        f.savefig(pig_name +'slope_change_mark.pdf')\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "    return x[max_idx], max_idx\n",
        "\n",
        "\n",
        "def bootstrap(b_data, n_rep, func=np.mean):\n",
        "\n",
        "    \"\"\"\n",
        "    Generate `n` bootstrap samples, evaluating `func`\n",
        "    at each resampling. `bootstrap` returns a function,\n",
        "    which can be called to obtain confidence intervals\n",
        "    of interest.\n",
        "    \"\"\"\n",
        "    simulations = list()\n",
        "    sample_size = len(b_data)\n",
        "    #     xbar_init = np.mean(data)\n",
        "    xbar_init = np.var(b_data)\n",
        "    for c in range(n_rep):\n",
        "        itersample = np.random.choice(b_data, size=sample_size, replace=True)\n",
        "        simulations.append(func(itersample))\n",
        "    simulations.sort()\n",
        "\n",
        "    def ci(p):\n",
        "        \"\"\"\n",
        "        Return 2-sided symmetric confidence interval specified\n",
        "        by p.\n",
        "        \"\"\"\n",
        "        u_pval = (1 + p) / 2.\n",
        "        l_pval = (1 - u_pval)\n",
        "        l_indx = int(np.floor(n_rep * l_pval))\n",
        "        u_indx = int(np.floor(n_rep * u_pval))\n",
        "        return (simulations[l_indx], simulations[u_indx])\n",
        "\n",
        "    return (ci)\n",
        "\n",
        "\n",
        "def bootstrap_out(c_data,c_idx,f_name, rep):\n",
        "    total_value = []\n",
        "    total_value2 = []\n",
        "    total_value3 = []\n",
        "\n",
        "    # copy_data = c.deepcopy(x[max_idx])\n",
        "    sample = c_data[0]\n",
        "\n",
        "    for i in range(0, c_data.shape[0]):\n",
        "        sample = np.append(sample, c_data[i])\n",
        "\t\t\n",
        "        var_sample = sample.var()\n",
        "        #     var_sample = s.mean(sample)\n",
        "        boot = bootstrap(sample, rep)\n",
        "\t\t# len(cintervals)= 3\n",
        "        cintervals = [boot(j) for j in (.90, .95, .99)]\n",
        "\n",
        "        #         cintervals = [boot(i) for i in range(len(ci_range)-1) ]\n",
        "        k = 0\n",
        "        while k <= len(cintervals) - 1:\n",
        "            # check whther a var is in the CI\n",
        "            if (var_sample >= cintervals[k][0]) & (var_sample <= cintervals[k][1]):\n",
        "                out = 1  ### print('Y')\n",
        "            else:\n",
        "                out = 0\n",
        "\n",
        "            if k == 0:\n",
        "                out_value = [i, var_sample, cintervals[k][0], cintervals[k][1], out]\n",
        "                total_value.append(out_value)\n",
        "            elif k == 1:\n",
        "                out_value2 = [cintervals[k][0], cintervals[k][1], out]\n",
        "                total_value2.append(out_value2)\n",
        "            else:\n",
        "                out_value3 = [cintervals[k][0], cintervals[k][1], out]\n",
        "                total_value3.append(out_value3)\n",
        "\n",
        "            k = k + 1\n",
        "    #     out_value = [i,cintervals[0][0],cintervals[0][1],cintervals[1][0],cintervals[1][1],cintervals[2][0],cintervals[2][1]]\n",
        "\n",
        "    #     out_value = [i,var_sample,ci_sample[0],ci_sample[1],out]\n",
        "    total_value = pd.DataFrame(total_value)\n",
        "    total_value2 = pd.DataFrame(total_value2)\n",
        "    total_value3 = pd.DataFrame(total_value3)\n",
        "    cg_idx = pd.DataFrame(c_idx[1:]) # index for change point\n",
        "\n",
        "    all_CIs = pd.concat([cg_idx, total_value, total_value2, total_value3], axis=1)\n",
        "    all_CIs.columns  = ['id', 'idx', 'var','.9Low', '.9up', 'out_9', '.95L', '.95U', 'out_95', '.99L', '.99U', 'out_99']\n",
        "    out_name= f_name + '_' + str(rep) + '_bootstrapC'\n",
        "    out_file_name = \"%s.csv\" % out_name\n",
        "    # all_CIs=pd.DataFrame(all_CIs,columns=['idx','.9Low','.9up','out_9','.95L','.95U','out_95','.99L','.99U','out_99',])\n",
        "    all_CIs.to_csv(out_file_name, index=False)\n",
        "    # all_CIs.to_csv('bootstrapVar5000_CI_ratio.csv', index=False)\n",
        "\n",
        "    return all_CIs"
      ],
      "metadata": {
        "id": "yTagPncKq1Ig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}